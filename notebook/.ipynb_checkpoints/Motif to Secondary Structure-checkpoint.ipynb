{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from make_representations.cpe_apply import CPE\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from proteinseq_util.biophysical import ProtSeqProp\n",
    "from utility.math_utility import normalize_mat\n",
    "import scipy.stats as st\n",
    "from chi2analysis.chi2analysis import Chi2Analysis\n",
    "from utility.math_utility import get_sym_kl_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class HierarchicalClutering(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, distance_matrix, labels_out):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        z = hac.complete(distance_matrix)\n",
    "        hac.dendrogram(z,labels=labels_out)\n",
    "        self.tree = hac.to_tree(z,False)\n",
    "        self.nwk=self.get_newick(self.tree, \"\", self.tree.dist, labels_out)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def get_newick(self, node, newick, parentdist, leaf_names):\n",
    "        '''\n",
    "        :param node:\n",
    "        :param newick:\n",
    "        :param parentdist:\n",
    "        :param leaf_names:\n",
    "        :return: the Newick format based on the provided distance matrix\n",
    "        '''\n",
    "        if node.is_leaf():\n",
    "            return \"%s:%.2f%s\" % (leaf_names[node.id], parentdist - node.dist, newick)\n",
    "        else:\n",
    "            if len(newick) > 0:\n",
    "                newick = \"):%.2f%s\" % (parentdist - node.dist, newick)\n",
    "            else:\n",
    "                newick = \");\"\n",
    "            newick = self.get_newick(node.get_left(), newick, node.dist, leaf_names)\n",
    "            newick = self.get_newick(node.get_right(), \",%s\" % (newick), node.dist, leaf_names)\n",
    "            newick = \"(%s\" % (newick)\n",
    "            return newick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiMotif(object):\n",
    "    def __init__(self, pos_fasta, neg_fasta, output_path, segmentation_schemes=10, topN=100):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        self.pos=FileUtility.read_fasta_sequences(pos_fasta)[0:10]\n",
    "        self.neg=FileUtility.read_fasta_sequences(neg_fasta)[0:10]\n",
    "        self.seqs=[seq.lower() for seq in self.pos+self.neg]\n",
    "        self.labels=[1]*len(self.pos)+[0]*len(self.neg)\n",
    "        self.segmentation_schemes=segmentation_schemes\n",
    "        self.load_alpha_distribution()\n",
    "        self.prepare_segmentations()\n",
    "        FileUtility.ensure_dir(output_path)\n",
    "        self.output_path=output_path\n",
    "        self.motif_extraction(topN)\n",
    "        \n",
    "    def load_alpha_distribution(self):\n",
    "        swiss_size_change=FileUtility.load_obj('../data_config//swiss_1000_samples.pickle')\n",
    "        all_samples=[]\n",
    "        for i in tqdm.tqdm(range(0,1000)):\n",
    "            sample=[]\n",
    "            for vocab in np.arange(10000,1000000,10000):\n",
    "                sample.append(size_change[vocab][i])\n",
    "            all_samples.append(-np.diff(sample))\n",
    "\n",
    "        sample_mat=np.mean(normalize_mat(all_samples),axis=0)\n",
    "        sample_mat_std=np.std(normalize_mat(all_samples),axis=0)\n",
    "        self.alpha_param = st.alpha.fit(sample_mat)\n",
    "    \n",
    "    def get_alpha_samples(self):\n",
    "        r = st.alpha.rvs(self.alpha_param[0], size=self.segmentation_schemes)\n",
    "        idx=np.array(np.round(10000+(r*10000)),dtype=np.int32).tolist()\n",
    "        idx.sort()\n",
    "        return idx\n",
    "\n",
    "    def prepare_segmentations(self):\n",
    "        segmented_seqs=[]\n",
    "        vocab_sizes=self.get_alpha_samples()\n",
    "        for i, vocab in tqdm.tqdm(enumerate(vocab_sizes)):\n",
    "            f=open('../data_config/swissprot_ppe','r')\n",
    "            CPE_Applier=CPE(f,separator='', merge_size=vocab)\n",
    "            for idx, seq in tqdm.tqdm(enumerate(self.seqs)):\n",
    "                if i ==0:\n",
    "                    segmented_seqs.append([CPE_Applier.segment(seq)])\n",
    "                else:\n",
    "                    segmented_seqs[idx]+=[CPE_Applier.segment(seq)]\n",
    "        self.extended_sequences=[' '.join(l) for l in segmented_seqs]\n",
    "    \n",
    "    \n",
    "    def motif_extraction(self, topn=100):\n",
    "        cpe_vectorizer = TfidfVectorizer(use_idf=False, analyzer='word',\n",
    "                                              norm=None, stop_words=[], lowercase=True, binary=False, tokenizer=str.split)\n",
    "        tf_vec=cpe_vectorizer.fit_transform(self.extended_sequences)\n",
    "        vocab=cpe_vectorizer.get_feature_names()\n",
    "        CH=Chi2Analysis(tf_vec,labels,vocab)\n",
    "        vocab_binary=[x[0] for x in CH.extract_features_fdr(self.output_path+'/motifs.txt', N=topn, alpha=5e-2, direction=True, allow_subseq=True, binarization=True, remove_redundant_markers=False) if x[1]>0]\n",
    "        idxs=[vocab.index(v) for v in vocab_binary]\n",
    "        pos_matrix=tf_vec.toarray()[0:len(self.pos),idx]\n",
    "        DIST=get_sym_kl_rows(pos_matrix.T)\n",
    "        HC=HierarchicalClutering(self.DIST,vocab_binary)\n",
    "        self.motifs=vocab_binary\n",
    "        self.tree=HC.nwk\n",
    "        FileUtility.save_list(self.output_path+'/motif_tree.txt', [HC.nwk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 26340.01it/s]\n",
      "/mounts/Users/student/asgari/.local/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:2306: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  Lhat = muhat - Shat*mu\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 13.39it/s]\u001b[A\n",
      "4it [00:00, 10.66it/s]\u001b[A\n",
      "5it [00:00,  9.51it/s]\u001b[A\n",
      "6it [00:00,  8.93it/s]\u001b[A\n",
      "7it [00:00,  8.60it/s]\u001b[A\n",
      "8it [00:00,  8.33it/s]\u001b[A\n",
      "9it [00:01,  7.48it/s]\u001b[A\n",
      "10it [00:01,  7.41it/s]\u001b[A\n",
      "13it [00:01,  8.76it/s]\u001b[A\n",
      "15it [00:01,  9.47it/s]\u001b[A\n",
      "18it [00:01, 10.44it/s]\u001b[A\n",
      "1it [00:02,  2.34s/it]]\u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 13.13it/s]\u001b[A\n",
      "4it [00:00, 10.43it/s]\u001b[A\n",
      "5it [00:00,  9.32it/s]\u001b[A\n",
      "6it [00:00,  8.75it/s]\u001b[A\n",
      "7it [00:00,  8.42it/s]\u001b[A\n",
      "8it [00:00,  8.15it/s]\u001b[A\n",
      "9it [00:01,  7.28it/s]\u001b[A\n",
      "10it [00:01,  7.22it/s]\u001b[A\n",
      "13it [00:01,  8.54it/s]\u001b[A\n",
      "15it [00:01,  9.24it/s]\u001b[A\n",
      "18it [00:01, 10.20it/s]\u001b[A\n",
      "2it [00:04,  2.36s/it]]\u001b[A\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 13.00it/s]\u001b[A\n",
      "4it [00:00, 10.37it/s]\u001b[A\n",
      "5it [00:00,  9.23it/s]\u001b[A\n",
      "6it [00:00,  8.65it/s]\u001b[A\n",
      "7it [00:00,  8.32it/s]\u001b[A\n",
      "8it [00:00,  8.06it/s]\u001b[A\n",
      "9it [00:01,  7.22it/s]\u001b[A\n",
      "10it [00:01,  7.16it/s]\u001b[A\n",
      "13it [00:01,  8.47it/s]\u001b[A\n",
      "15it [00:01,  9.16it/s]\u001b[A\n",
      "17it [00:01,  9.77it/s]\u001b[A\n",
      "20it [00:01, 10.75it/s]\u001b[A\n",
      "3it [00:07,  2.37s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 12.98it/s]\u001b[A\n",
      "4it [00:00, 10.38it/s]\u001b[A\n",
      "5it [00:00,  9.23it/s]\u001b[A\n",
      "6it [00:00,  8.64it/s]\u001b[A\n",
      "7it [00:00,  8.29it/s]\u001b[A\n",
      "8it [00:00,  8.03it/s]\u001b[A\n",
      "9it [00:01,  7.18it/s]\u001b[A\n",
      "10it [00:01,  7.12it/s]\u001b[A\n",
      "13it [00:01,  8.44it/s]\u001b[A\n",
      "15it [00:01,  9.13it/s]\u001b[A\n",
      "17it [00:01,  9.74it/s]\u001b[A\n",
      "20it [00:01, 10.72it/s]\u001b[A\n",
      "4it [00:09,  2.38s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 13.01it/s]\u001b[A\n",
      "4it [00:00, 10.37it/s]\u001b[A\n",
      "5it [00:00,  9.24it/s]\u001b[A\n",
      "6it [00:00,  8.65it/s]\u001b[A\n",
      "7it [00:00,  8.31it/s]\u001b[A\n",
      "8it [00:00,  8.06it/s]\u001b[A\n",
      "9it [00:01,  7.21it/s]\u001b[A\n",
      "10it [00:01,  7.15it/s]\u001b[A\n",
      "13it [00:01,  8.46it/s]\u001b[A\n",
      "15it [00:01,  9.15it/s]\u001b[A\n",
      "17it [00:01,  9.77it/s]\u001b[A\n",
      "20it [00:01, 10.74it/s]\u001b[A\n",
      "5it [00:11,  2.39s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 12.55it/s]\u001b[A\n",
      "4it [00:00, 10.03it/s]\u001b[A\n",
      "5it [00:00,  8.95it/s]\u001b[A\n",
      "6it [00:00,  8.40it/s]\u001b[A\n",
      "7it [00:00,  8.05it/s]\u001b[A\n",
      "8it [00:01,  7.83it/s]\u001b[A\n",
      "9it [00:01,  6.99it/s]\u001b[A\n",
      "10it [00:01,  6.83it/s]\u001b[A\n",
      "13it [00:01,  8.11it/s]\u001b[A\n",
      "15it [00:01,  8.79it/s]\u001b[A\n",
      "17it [00:01,  9.40it/s]\u001b[A\n",
      "20it [00:01, 10.36it/s]\u001b[A\n",
      "6it [00:14,  2.40s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 12.75it/s]\u001b[A\n",
      "4it [00:00, 10.17it/s]\u001b[A\n",
      "5it [00:00,  9.06it/s]\u001b[A\n",
      "6it [00:00,  8.49it/s]\u001b[A\n",
      "7it [00:00,  8.16it/s]\u001b[A\n",
      "8it [00:01,  7.91it/s]\u001b[A\n",
      "9it [00:01,  7.07it/s]\u001b[A\n",
      "10it [00:01,  7.02it/s]\u001b[A\n",
      "13it [00:01,  8.32it/s]\u001b[A\n",
      "15it [00:01,  9.00it/s]\u001b[A\n",
      "17it [00:01,  9.61it/s]\u001b[A\n",
      "20it [00:01, 10.58it/s]\u001b[A\n",
      "7it [00:16,  2.40s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 12.35it/s]\u001b[A\n",
      "4it [00:00,  9.87it/s]\u001b[A\n",
      "5it [00:00,  8.76it/s]\u001b[A\n",
      "6it [00:00,  8.21it/s]\u001b[A\n",
      "7it [00:00,  7.90it/s]\u001b[A\n",
      "8it [00:01,  7.66it/s]\u001b[A\n",
      "9it [00:01,  6.81it/s]\u001b[A\n",
      "10it [00:01,  6.76it/s]\u001b[A\n",
      "12it [00:01,  7.60it/s]\u001b[A\n",
      "15it [00:01,  8.68it/s]\u001b[A\n",
      "17it [00:01,  9.27it/s]\u001b[A\n",
      "20it [00:01, 10.22it/s]\u001b[A\n",
      "8it [00:19,  2.42s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 12.58it/s]\u001b[A\n",
      "4it [00:00, 10.02it/s]\u001b[A\n",
      "5it [00:00,  8.90it/s]\u001b[A\n",
      "6it [00:00,  8.31it/s]\u001b[A\n",
      "7it [00:00,  7.97it/s]\u001b[A\n",
      "8it [00:01,  7.72it/s]\u001b[A\n",
      "9it [00:01,  6.86it/s]\u001b[A\n",
      "10it [00:01,  6.80it/s]\u001b[A\n",
      "12it [00:01,  7.64it/s]\u001b[A\n",
      "15it [00:01,  8.72it/s]\u001b[A\n",
      "17it [00:01,  9.31it/s]\u001b[A\n",
      "20it [00:01, 10.26it/s]\u001b[A\n",
      "9it [00:21,  2.43s/it]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "3it [00:00, 12.01it/s]\u001b[A\n",
      "4it [00:00,  9.58it/s]\u001b[A\n",
      "5it [00:00,  8.47it/s]\u001b[A\n",
      "6it [00:00,  7.92it/s]\u001b[A\n",
      "7it [00:00,  7.61it/s]\u001b[A\n",
      "8it [00:01,  7.39it/s]\u001b[A\n",
      "9it [00:01,  6.59it/s]\u001b[A\n",
      "10it [00:01,  6.55it/s]\u001b[A\n",
      "12it [00:01,  7.36it/s]\u001b[A\n",
      "15it [00:01,  8.43it/s]\u001b[A\n",
      "17it [00:01,  9.01it/s]\u001b[A\n",
      "20it [00:02,  9.93it/s]\u001b[A\n",
      "10it [00:24,  2.44s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-dbbc6f2df853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDiMotif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/mounts/data/proj/asgari/dissertation/git_repos/less_important/protein_datasets_new/integrins/positive_integrins.fasta'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'/mounts/data/proj/asgari/dissertation/git_repos/less_important/protein_datasets_new/integrins/neg_reviewed.fasta'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'./integrins/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-1f477a28c0b3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pos_fasta, neg_fasta, output_path, segmentation_schemes)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mFileUtility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotif_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_alpha_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-1f477a28c0b3>\u001b[0m in \u001b[0;36mmotif_extraction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmotif_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0msegmented\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         cpe_vectorizer = TfidfVectorizer(use_idf=False, analyzer='word',\n\u001b[1;32m     53\u001b[0m                                               norm=None, stop_words=[], lowercase=True, binary=False, tokenizer=str.split)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "DM=DiMotif('/mounts/data/proj/asgari/dissertation/git_repos/less_important/protein_datasets_new/integrins/positive_integrins.fasta','/mounts/data/proj/asgari/dissertation/git_repos/less_important/protein_datasets_new/integrins/neg_reviewed.fasta','./integrins/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(DM.extended_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 992.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2480.37it/s]\n"
     ]
    }
   ],
   "source": [
    "a,b=MotifProp.get_motifs_pss_biophys(['dve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utility.file_utility import FileUtility\n",
    "from make_representations.cpe_apply import CPE\n",
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from chi2analysis.chi2analysis import Chi2Analysis\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes=[int(x) for x in FileUtility.load_list('../../protein_datasets/segmentations/alpha_sampled_list.txt')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
